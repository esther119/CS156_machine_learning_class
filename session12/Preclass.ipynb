{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binomial Distribution \n",
    "$P(x:n,p) = nCx p^x (1-p)^{(n-x)}$\n",
    "- n is the number of trials (occurrences)\n",
    "- k is the number of successful trials\n",
    "- p is probability of success in a single trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "heads = [14, 33, 19, 10, 0, 17, 24, 17, 1, 36, 5, 6, 5, 13, 4, 35, 5, 5, 74, 34]\n",
    "throws = [41, 43, 23, 23, 1, 23, 36, 37, 2, 131, 5, 29, 13, 47, 10, 58, 15, 14, 100, 113]\n",
    "tails = [throws[i] - heads[i] for i in range(len(throws))]\n",
    "\n",
    "\n",
    "# xs = np.array([heads, heads])\n",
    "xs = list(zip(heads, tails))\n",
    "thetas = np.array([[0.5001, 0.4999], [0.5, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "theta_A = 0.47, theta_B = 0.47, log likelihood = -529.57\n",
      "Iteration: 2\n",
      "theta_A = 0.47, theta_B = 0.46, log likelihood = -527.91\n",
      "Iteration: 3\n",
      "theta_A = 0.55, theta_B = 0.39, log likelihood = -526.18\n",
      "Iteration: 4\n",
      "theta_A = 0.68, theta_B = 0.31, log likelihood = -492.69\n",
      "Iteration: 5\n",
      "theta_A = 0.71, theta_B = 0.31, log likelihood = -470.33\n",
      "Iteration: 6\n",
      "theta_A = 0.71, theta_B = 0.31, log likelihood = -469.04\n",
      "Iteration: 7\n",
      "theta_A = 0.71, theta_B = 0.31, log likelihood = -468.89\n",
      "Iteration: 8\n",
      "theta_A = 0.71, theta_B = 0.31, log likelihood = -468.88\n",
      "Iteration: 9\n",
      "theta_A = 0.71, theta_B = 0.31, log likelihood = -468.87\n",
      "Iteration: 10\n",
      "theta_A = 0.71, theta_B = 0.31, log likelihood = -468.87\n",
      "Iteration: 11\n",
      "theta_A = 0.71, theta_B = 0.31, log likelihood = -468.87\n"
     ]
    }
   ],
   "source": [
    "tol = 0.0001\n",
    "max_iter = 100\n",
    "\n",
    "def EM(xs, thetas ,tol, max_iter):\n",
    "    ll_old = 0\n",
    "    for i in range(max_iter):\n",
    "        ws_A = []\n",
    "        ws_B = []\n",
    "\n",
    "        vs_A = []\n",
    "        vs_B = []\n",
    "\n",
    "        ll_new = 0\n",
    "\n",
    "        # E-step: calculate probability distributions over possible completions\n",
    "        for x in xs:\n",
    "\n",
    "            # multinomial (binomial) log likelihood\n",
    "            #reason: log (theta^p *(1-theta)^q) = p*log(theta) + q*log(1-theta) \n",
    "            ll_A = np.sum([x*np.log(thetas[0])]) \n",
    "            ll_B = np.sum([x*np.log(thetas[1])])\n",
    "\n",
    "            # [EQN 1]: weight \n",
    "            denom = np.exp(ll_A) + np.exp(ll_B)\n",
    "            w_A = np.exp(ll_A)/denom\n",
    "            w_B = np.exp(ll_B)/denom\n",
    "\n",
    "            ws_A.append(w_A)\n",
    "            ws_B.append(w_B)\n",
    "\n",
    "            # expected head, tails \n",
    "            vs_A.append(np.dot(w_A, x))\n",
    "            vs_B.append(np.dot(w_B, x))\n",
    "\n",
    "            # update complete log likelihood\n",
    "            ll_new += w_A * ll_A + w_B * ll_B\n",
    "\n",
    "        # M-step: update values for parameters given current distribution\n",
    "        # [EQN 2]\n",
    "        thetas[0] = np.sum(vs_A, 0)/np.sum(vs_A)\n",
    "        thetas[1] = np.sum(vs_B, 0)/np.sum(vs_B)\n",
    "        # print distribution of z for each x and current parameter estimate\n",
    "\n",
    "        print(\"Iteration: %d\" % (i+1))\n",
    "        print(\"theta_A = %.2f, theta_B = %.2f, log likelihood = %.2f\" % (thetas[0,0], thetas[1,0], ll_new))\n",
    "\n",
    "        if np.abs(ll_new - ll_old) < tol:\n",
    "            break\n",
    "        ll_old = ll_new\n",
    "EM(xs, thetas ,tol, max_iter)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) There could be an optional bias parameter in this model, if it is much more likely to pick one coin. Update your model to also estimate the bias\n",
    "\n",
    "- average the expected value for coin A and coin B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.32846026694949065, 0.7632534563283143)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_coin = []\n",
    "five_coin = []\n",
    "ratio = np.divide(heads,throws)\n",
    "for i in range(len(ratio)): \n",
    "    if ratio[i] > 0.5:\n",
    "        five_coin.append(ratio[i])\n",
    "    else: \n",
    "        three_coin.append(ratio[i])\n",
    "np.mean(three_coin), np.mean(five_coin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
