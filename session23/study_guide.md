# Markov models

![markov1.jpg](https://github.com/minerva-university/cs156/blob/master/session23/markov1.jpg)

Try to think for yourself first whether you understand the differences of the following terms. Then check on the next page whether your understanding is correct.
Markov property, Markov assumption, Markov model, Markov chain, Markov process, initial distribution, current state, transition matrix, equilibrium and stationary distribution, (non-) stationary Markov chain

Check out [Markov Chains in Python: Beginner Tutorial](https://www.datacamp.com/community/tutorials/markov-chains-python-tutorial) if you’d like to know more (some of the study guide comes from here). 


- Markov property - The probability of transitioning to any particular state is dependent solely on the current state, and not on previous states (“given the present, the future does not depend on the past”, “memoryless”, Pr( Xn+1 = x | X1 = x1, X2 = x2, …, Xn = xn) = Pr( Xn+1 = x | Xn = xn)).
- Markov assumption - The Markov assumption assumes the markov property (memoryless).
- Markov model - A model with the Markov property.
- Markov chain - A collection of random variables, that transition from one state to another according to probabilistic rules. The random variable can take the form of a letter, number, weather, … but for a Markov chain it is countable/ discrete.
- Markov process - Same as a Markov chain, but it can be continuous or discrete.  
- Initial distribution - The initial distribution is the probability distribution over states (e.g. sleep, run, ice cream) that the starting state (e.g. ice cream) is sampled from. 
- Current state - The current state is the probability distribution over states at the current time. 
- Transition matrix - Each row and column correspond to a state the random variable can take. A cell contains the probability of transitioning from the row state to the column state. All values in a row add up to 1.

![markov2.png](https://github.com/minerva-university/cs156/blob/master/session23/markov2.png)

- Equilibrium and stationary distribution - A Markov chain has an equilibrium distribution if eventually the distribution over states does not change and does not depend on the initial state. 
  $\pi P = \pi$, with transition matrix P and stationary distribution $\pi$
If $\pi$  is an equilibrium distribution, it is a stationary distribution for the given markov chain. (Read more on the distinction between equilibrium and stationary distribution [here](http://edshare.soton.ac.uk/2117/6/Ma222exqu13.pdf) or 23.1.1 in Barber.)
- (Non-) Stationary Markov chain - A classic Markov chain is stationary, meaning the transition matrix does not change over time. In a non-stationary Markov chain the transition matrix can change over time.



## Pre-Class Work
There are three languages: A, B, and C. Each language uses the same set of symbols: “A, o, e, t, p, g, and k. However, each language uses the symbols differently. In each of these languages we can model everything as P(next symbol | current symbol).
There is training data available for each language. This consists of several files each generated by sampling from a Markov model. Using python, build a Markov model for each of the languages. Now use the Markov model and Bayes’ rule to classify the test cases. Write down how you used Bayes’ rule to get your classifier. Give the full posterior distribution for each test case.

### For pre-class work you are asked to do two things:

1. build a Markov model for each language:
    1. find the initial distribution (With what probability does each letter occur first?)
    2. find the transition matrix (Look [here](https://stackoverflow.com/questions/46657221/generating-markov-transition-matrix-in-python) if you’re stuck.)
    3. Now you have a Markov model :)
2. classify test cases for each language using Bayes’ rule → find the probability of a language, given the string: P(language = A|string = “Aoetpp”)
    1. Bayes rule: P(A|B) = $\frac{P(B|A)*P(A)}{P(B)}$
    2. P(language = A|string = “Aoetpp”) is the posterior we are interested in.
    3. P(string = “Aoetpp”|language = A) is the likelihood, the probability of a string given a certain language
        1. calculate the probability of the markov model for language A generating the string “Aoetpp”
        2. What is the probability of “A”? (initial distribution)
        3. What is the probability of “o” given “A”? (transition matrix)
    4. P(language = A) is the prior probability we have about the probability of the language. (You can assume a uniform prior.)
    5. P(string = “Aoetpp”) is the evidence, or the probability of the string “Aoetpp”.
        1. P(string = “Aoetpp”|language = A) * P(language = A) 
            + P(string = “Aoetpp”|language = B) * P(language = B) 
            + P(string = “Aoetpp”|language = C) * P(language = C)
3. Identify which language model has the highest probability … tadaaaam!
